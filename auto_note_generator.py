import os
import sys
import shutil
import subprocess
import time
import uuid
import json
import torch
import difflib # ç¡®ä¿difflibè¢«å¯¼å…¥
from openai import OpenAI
from faster_whisper import WhisperModel

# å…¨å±€é…ç½®åŒº 
FFMPEG_PATH = "ffmpeg"
EVP_PATH = "evp"
API_KEY = os.getenv("SILICON_CLOUD_API_KEY")
BASE_URL = "https://api.siliconflow.cn/v1"
MODEL_NAME = "Qwen/Qwen3-30B-A3B-Thinking-2507"
FASTER_WHISPER_MODEL_PATH = r"C:\Users\ZzZz\.cache\modelscope\hub\models\angelala00\faster-whisper-small" # è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®

# --- [ä¿®æ­£] å°†ç¯å¢ƒæ£€æŸ¥å’Œå˜é‡å®šä¹‰ç§»åˆ°æ¨¡å—é¡¶å±‚ ---
print("--- æ­£åœ¨åˆå§‹åŒ–æ¨¡å—å¹¶æ£€æŸ¥è¿è¡Œç¯å¢ƒ ---")
if torch.cuda.is_available():
    DEVICE = "cuda"
    COMPUTE_TYPE = "float16"
    print("CUDA (GPU) å¯ç”¨ï¼å°†ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿã€‚")
else:
    DEVICE = "cpu"
    COMPUTE_TYPE = "int8"
    print("CUDA (GPU) ä¸å¯ç”¨ã€‚å°†ä½¿ç”¨CPUè¿è¡Œï¼Œé€Ÿåº¦ä¼šè¾ƒæ…¢ã€‚")
print("---------------------------------------\n")


# Prompt ---
GLOBAL_OPTIMIZE_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„æ–‡æœ¬ä¿®å¤å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸€ä»½å®Œæ•´çš„ã€ç”±è¯­éŸ³è¯†åˆ«ç”Ÿæˆçš„è¯¾å ‚æ•™å­¦åŸå§‹æ–‡ç¨¿ï¼Œè½¬åŒ–ä¸ºä¸€ç¯‡æµç•…çš„æ–‡ç« ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š

1.  **é€šè¯»å…¨æ–‡ï¼Œç†è§£ä¸Šä¸‹æ–‡**ï¼šåœ¨ä¿®æ­£ä»»ä½•å¥å­ä¹‹å‰ï¼Œå…ˆç†è§£æ•´ä¸ªæ®µè½ä¹ƒè‡³å…¨æ–‡çš„ä¸»æ—¨å’Œé€»è¾‘ã€‚
2.  **æ¸…é™¤æ‰€æœ‰å£è¯­åŒ–ç—•è¿¹**ï¼šå½»åº•åˆ é™¤æ‰€æœ‰æ— æ„ä¹‰çš„è¯­æ°”è¯ï¼ˆå¦‚â€œå—¯â€ã€â€œå•Šâ€ã€â€œé‚£ä¸ªâ€ï¼‰ã€ä¸å¿…è¦çš„é‡å¤å’ŒçŠ¹è±«ã€‚
3.  **ä¿æŒåŸæ„ä¸æœ¯è¯­**ï¼šè¿™æ˜¯æœ€é‡è¦çš„è§„åˆ™ã€‚ä½ åªèƒ½åšâ€œä¿®æ­£â€ï¼Œç»å¯¹ä¸èƒ½æ·»åŠ è‡ªå·±çš„è§‚ç‚¹ã€è¿›è¡Œå†…å®¹æ€»ç»“æˆ–åˆ é™¤ä»»ä½•å…³é”®ä¿¡æ¯å’Œä¸“ä¸šæœ¯è¯­ã€‚
4.  **è¾“å‡ºçº¯å‡€çš„æ–‡æœ¬**ï¼šåªè¾“å‡ºä¿®å¤åçš„å®Œæ•´æ–‡ç« ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰è¨€ã€æ ‡é¢˜ã€æ‘˜è¦æˆ–è¯„è®ºã€‚

åŸå§‹æ–‡ç¨¿å¦‚ä¸‹ï¼š
---
{full_raw_speech}
"""



def run_command(command, description):

    print(f"--- æ­£åœ¨æ‰§è¡Œ: {description} ---")
    print(f"CMD: {' '.join(command)}")
    try:
        subprocess.run(command, check=True, capture_output=True)
        print(f"--- {description}... æˆåŠŸ ---")
        return True
    except subprocess.CalledProcessError as e:
        error_message = e.stderr.decode(sys.getdefaultencoding(), errors='ignore')
        print(f"!!! é”™è¯¯: {description} å¤±è´¥ã€‚\n{error_message}")
        return False
    except FileNotFoundError:
        print(f"!!! é”™è¯¯: å‘½ä»¤ '{command[0]}' æœªæ‰¾åˆ°ã€‚")
        return False

# --- æ–‡æœ¬ä¼˜åŒ–å‡½æ•°
def optimize_full_text(client, full_text):
    if not full_text: return ""
    print("\n--- æ­£åœ¨å¯¹å…¨æ–‡è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¼˜åŒ– (è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´) ---")
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": GLOBAL_OPTIMIZE_PROMPT.format(full_raw_speech=full_text)}],
            temperature=0.2,
            stream=False
        )
        print("--- å…¨æ–‡ä¼˜åŒ–å®Œæˆ ---")
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"\n!!! å…¨æ–‡ä¼˜åŒ–APIè°ƒç”¨å¤±è´¥: {e}")
        return None

# --- æ—¶é—´æˆ³å¯¹é½å‡½æ•° ---
def align_timestamps(raw_words_with_ts, optimized_text):
    print("--- æ­£åœ¨å°†æ—¶é—´æˆ³æ˜ å°„åˆ°ä¼˜åŒ–åçš„æ–‡æœ¬ ---")
    raw_text = "".join(w['word'] for w in raw_words_with_ts)
    
    optimized_words_with_ts = []
    
    # åˆ›å»ºä¸€ä¸ªä»åŸå§‹æ–‡æœ¬å­—ç¬¦ç´¢å¼•åˆ°å•è¯ä¿¡æ¯ï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰çš„æ˜ å°„
    # è¿™æ¯”ä¹‹å‰çš„char_to_word_mapæ›´ç›´æ¥é«˜æ•ˆ
    char_to_word_map = {}
    char_cursor = 0
    for word_info in raw_words_with_ts:
        for _ in word_info['word']:
            char_to_word_map[char_cursor] = word_info
            char_cursor += 1

    matcher = difflib.SequenceMatcher(None, raw_text, optimized_text, autojunk=False)
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            # æ–‡å­—ç›¸åŒï¼Œç›´æ¥ç»§æ‰¿æ—¶é—´æˆ³
            for raw_char_index in range(i1, i2):
                if raw_char_index in char_to_word_map:
                    source_word = char_to_word_map[raw_char_index]
                    # ä»ä¼˜åŒ–æ–‡æœ¬ä¸­å–å‡ºå¯¹åº”çš„å­—ç¬¦
                    optimized_char = optimized_text[j1 + (raw_char_index - i1)]
                    optimized_words_with_ts.append({
                        'word': optimized_char, 
                        'start': source_word['start'], 
                        'end': source_word['end']
                    })
        
        elif tag == 'replace' or tag == 'insert':
            # --- [æ ¸å¿ƒä¿®å¤é€»è¾‘] ---
            # è¿™æ˜¯è¢«æ¨¡å‹ä¿®æ”¹æˆ–æ–°å¢çš„æ–‡æœ¬ï¼Œéœ€è¦æ™ºèƒ½åœ°å¯»æ‰¾æ—¶é—´æˆ³é”šç‚¹

            # 1. ä¼˜å…ˆå¯»æ‰¾â€œåé”šç‚¹â€ï¼šå®ƒåé¢ç´§è·Ÿç€çš„åŸå§‹æ–‡æœ¬çš„ä½ç½®(i2)
            # è¿™å¯¹äºä¿®å¤å¥é¦–ç¼ºå¤±è‡³å…³é‡è¦
            anchor_char_index = i2
            
            # 2. å¦‚æœæ‰¾ä¸åˆ°â€œåé”šç‚¹â€ï¼ˆå³ä¿®æ”¹å‘ç”Ÿåœ¨æœ€æœ«å°¾ï¼‰ï¼Œåˆ™ä½¿ç”¨â€œå‰é”šç‚¹â€(i1 - 1)
            if anchor_char_index >= len(raw_text):
                anchor_char_index = i1 - 1

            # ç¡®ä¿é”šç‚¹ç´¢å¼•æœ‰æ•ˆ
            if 0 <= anchor_char_index < len(raw_text):
                anchor_word = char_to_word_map[anchor_char_index]
                # å°†è¿™æ®µæ–°æ–‡æœ¬ä¸­çš„æ¯ä¸ªå­—ï¼Œéƒ½èµ‹äºˆé”šç‚¹çš„æ—¶é—´æˆ³
                for char in optimized_text[j1:j2]:
                    optimized_words_with_ts.append({
                        'word': char, 
                        'start': anchor_word['start'], 
                        'end': anchor_word['end']
                    })
            else:
                # æç«¯æƒ…å†µï¼šå¦‚æœæ•´ä¸ªæ–‡æœ¬éƒ½è¢«æ›¿æ¢äº†ï¼Œå°±ç”¨ç¬¬ä¸€ä¸ªè¯çš„æ—¶é—´æˆ³
                anchor_word = raw_words_with_ts[0] if raw_words_with_ts else {'start': 0, 'end': 0}
                for char in optimized_text[j1:j2]:
                    optimized_words_with_ts.append({
                        'word': char, 
                        'start': anchor_word['start'], 
                        'end': anchor_word['end']
                    })

    print("--- æ—¶é—´æˆ³æ˜ å°„å®Œæˆ ---")
    return optimized_words_with_ts

def process_and_generate_final_note(image_dir, transcript_data, video_title):
    print("\n--- æ ¸å¿ƒå¤„ç†: æ­£åœ¨æ•´åˆå›¾æ–‡å¹¶ç”Ÿæˆç¬”è®° ---")
    actual_image_dir = os.path.join(image_dir, "frames")
    if not os.path.exists(actual_image_dir):
        print(f"!!! å…³é”®é”™è¯¯: é¢„æœŸçš„å›¾ç‰‡ç›®å½• '{actual_image_dir}' ä¸å­˜åœ¨ã€‚")
        return

    output_dir, final_md_path = "output", os.path.join("output", f"{video_title}_ç¬”è®°.md")
    image_files = [f for f in os.listdir(actual_image_dir) if f.lower().endswith('.jpg')]
    if not image_files:
        print("!!! å…³é”®é”™è¯¯: 'frames' å­ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•PPTå›¾ç‰‡ã€‚")
        return
        
    def filename_to_seconds(filename):
        parts = os.path.splitext(filename)[0].rstrip('-').split('.')
        return float(int(parts[0])*3600 + int(parts[1])*60 + int(parts[2]))
    ppt_timestamps = sorted([(filename_to_seconds(f), f) for f in image_files])
    
    raw_words_with_ts = [word for segment in transcript_data.get('segments', []) for word in segment.get('words', [])]
    full_raw_speech = "".join(w['word'] for w in raw_words_with_ts)

    if API_KEY and full_raw_speech:
        client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
        optimized_full_text = optimize_full_text(client, full_raw_speech)
        if optimized_full_text is None:
            print("è­¦å‘Š: å…¨æ–‡ä¼˜åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹æ–‡æœ¬ã€‚")
            optimized_full_text = full_raw_speech
    else:
        print("æœªé…ç½®API Keyæˆ–æ— è¯­éŸ³å†…å®¹ï¼Œè·³è¿‡æ–‡æœ¬ä¼˜åŒ–ã€‚")
        optimized_full_text = full_raw_speech

    optimized_words_with_ts = align_timestamps(raw_words_with_ts, optimized_full_text)

    video_duration = transcript_data['segments'][-1]['end'] if transcript_data.get('segments') else 0
    notes = []
    for i, (ppt_start_time, ppt_filename) in enumerate(ppt_timestamps):
        ppt_end_time = ppt_timestamps[i+1][0] if i + 1 < len(ppt_timestamps) else video_duration
        speech_text = "".join(word['word'] for word in optimized_words_with_ts if ppt_start_time <= word['start'] < ppt_end_time)
        notes.append({
            "ppt_path": os.path.join(actual_image_dir, ppt_filename), 
            "timestamp_str": os.path.splitext(ppt_filename)[0].rstrip('-').replace('.', ':'), 
            "speech": speech_text.strip()
        })
    
    os.makedirs(output_dir, exist_ok=True)
    with open(final_md_path, 'w', encoding='utf-8') as f:
        f.write(f"# {video_title} - æ•™å­¦ç¬”è®° (ç²¾ç‚¼ç‰ˆ)\n\n---\n\n")
        safe_title_for_dir = "".join(c for c in video_title if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
        final_img_base_dir = os.path.join(output_dir, "images", safe_title_for_dir)
        os.makedirs(final_img_base_dir, exist_ok=True)
        for i, note in enumerate(notes):
            img_name = os.path.basename(note['ppt_path'])
            shutil.copy(note['ppt_path'], os.path.join(final_img_base_dir, img_name))
            image_md_path = f"./images/{safe_title_for_dir}/{img_name}"
            f.write(f"## Slide {i+1} (æ—¶é—´ç‚¹: {note['timestamp_str']})\n\n![Slide {i+1}]({image_md_path})\n\n")

            # --- [æœ€ç»ˆä¿®æ­£] å¢åŠ åå¤„ç†æ­¥éª¤ ---
            speech_content = note['speech'] or '(æ­¤æ—¶é—´æ®µå†…æ— æ•™å¸ˆè®²ç¨¿)'
            # å°†æ‰€æœ‰æ¢è¡Œç¬¦æ›¿æ¢ä¸ºèƒ½è®©Markdownå¼•ç”¨å—æ­£ç¡®æ¢è¡Œçš„æ ¼å¼
            formatted_speech = speech_content.replace('\n', '\n> ')
            
            f.write(f"> {formatted_speech}\n\n---\n\n")

    print(f"ğŸ‰ æœ€ç»ˆä»»åŠ¡å®Œæˆï¼ç²¾ç‚¼ç‰ˆç¬”è®°å·²æˆåŠŸç”Ÿæˆäº: {final_md_path}")

def transcribe_audio_with_faster_whisper(audio_path, device, compute_type):

    print("\n--- æ­£åœ¨ä½¿ç”¨ faster-whisper è¿›è¡ŒéŸ³é¢‘è½¬æ–‡å­— ---")
    if not os.path.exists(FASTER_WHISPER_MODEL_PATH):
        print(f"!!! é”™è¯¯: faster-whisperæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {FASTER_WHISPER_MODEL_PATH}")
        return None
    print(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹åˆ° {device} (è®¡ç®—ç±»å‹: {compute_type})...")
    try:
        model = WhisperModel(FASTER_WHISPER_MODEL_PATH, device=device, compute_type=compute_type)
    except Exception as e:
        print(f"!!! åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None
    print("å¼€å§‹è½¬å½•...")
    segments_generator, info = model.transcribe(audio_path, language="zh", word_timestamps=True)
    whisper_data, total_segments = {"segments": [], "language": info.language}, 0
    for segment in segments_generator:
        total_segments += 1
        print(f"\ræ­£åœ¨å¤„ç†ç¬¬ {total_segments} æ®µè¯­éŸ³...", end="")
        words_list = segment.words or []
        whisper_data["segments"].append({"start": segment.start, "end": segment.end, "words": [{"word": w.word, "start": w.start, "end": w.end} for w in words_list]})
    print(f"\n--- éŸ³é¢‘è½¬æ–‡å­—å®Œæˆï¼Œå…±å¤„ç† {total_segments} æ®µã€‚---")
    return whisper_data


def main_pipeline(video_url, device, compute_type):
    # ... (ä»£ç ä¸å˜) ...
    temp_workspace = os.path.abspath(os.path.join("temp", str(uuid.uuid4())))
    os.makedirs(temp_workspace, exist_ok=True)
    print(f"åˆ›å»ºä¸´æ—¶å·¥ä½œåŒº: {temp_workspace}")

    try:
        print("\n--- æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯ ---")
        title_cmd = ['yt-dlp', '--get-title', '--no-warnings', '--skip-download', video_url]
        video_title = "Untitled_Video"
        try:
            title_result = subprocess.run(title_cmd, check=True, capture_output=True, timeout=20)
            for encoding in ['utf-8', sys.getdefaultencoding(), 'gbk']:
                try:
                    video_title = title_result.stdout.decode(encoding).strip()
                    break
                except UnicodeDecodeError: continue
        except Exception as e:
            print(f"è­¦å‘Šï¼šè·å–è§†é¢‘æ ‡é¢˜å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨é»˜è®¤æ ‡é¢˜ã€‚")
        
        safe_title = "".join(c for c in video_title if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
        if not safe_title: safe_title = "video_note_" + str(uuid.uuid4())[:8]
        print(f"è§†é¢‘æ ‡é¢˜å·²è¯†åˆ«ä¸º: {safe_title}")

        video_path = os.path.join(temp_workspace, "video.mp4")
        if not run_command(['yt-dlp', '-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best', '-o', video_path, video_url], "ä¸‹è½½å®Œæ•´è§†é¢‘"): return

        ppt_output_dir = os.path.join(temp_workspace, 'ppt_images')
        if not run_command([EVP_PATH, '--raw_frames', '--diff_threshold', '4', ppt_output_dir, video_path], "æå–PPTå›¾ç‰‡"): return

        audio_path = os.path.join(temp_workspace, 'audio.mp3')
        if not run_command([FFMPEG_PATH, '-i', video_path, '-q:a', '0', '-map', 'a', audio_path], "æå–éŸ³é¢‘"): return

        transcript_data = transcribe_audio_with_faster_whisper(audio_path, device, compute_type)
        if not transcript_data: return

        process_and_generate_final_note(ppt_output_dir, transcript_data, safe_title)

    finally:
        print(f"\n--- æ­£åœ¨æ¸…ç†ä¸´æ—¶å·¥ä½œåŒº: {temp_workspace} ---")
        if os.path.exists(temp_workspace):
            shutil.rmtree(temp_workspace)
        print("--- æ¸…ç†å®Œæˆ ---")


if __name__ == "__main__":
    url = sys.argv[1] if len(sys.argv) > 1 else input("è¯·è¾“å…¥Bç«™æ•™å­¦è§†é¢‘é“¾æ¥: ")
    if url.strip():
        main_pipeline(url, device=DEVICE, compute_type=COMPUTE_TYPE)
    else:
        print("é”™è¯¯ï¼šæœªè¾“å…¥é“¾æ¥ã€‚")